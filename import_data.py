# import_data.py

import pandas as pd
import sys
import logging
from tqdm import tqdm
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError, DataError
from datetime import date 
import os

# Assurez-vous que config.py et database_setup.py sont bien configur√©s
import config
import database_setup
from models import (
    Institution, Composante, Domaine, Mention, Parcours, 
    AnneeUniversitaire, Etudiant, Inscription,
    # CLASSES LMD & MODE INSCRIPTION
    Cycle, Niveau, Semestre, ModeInscription, SessionExamen,
    TypeFormation, ParcoursNiveau,
    # AJOUT POTENTIEL des nouvelles classes
    Enseignant, Jury
)

# Configuration du logging
logging.basicConfig(filename='import_errors.log', 
                    filemode='w', 
                    encoding='utf-8',
                    level=logging.ERROR,
                    format='%(asctime)s - %(levelname)s - %(message)s')


def safe_string(s):
    """Assure le nettoyage des cha√Ænes de caract√®res."""
    if s is None or not isinstance(s, str):
        return s
    
    s = str(s).strip()
    return s


# ----------------------------------------------------------------------
# G√âN√âRATION ET IMPORTATION DES DONN√âES DE R√âF√âRENCE FIXES
# ----------------------------------------------------------------------

def _generate_annee_data(start_year: int = 2021, end_year: int = 2026) -> list:
    """
    G√©n√®re les donn√©es des ann√©es universitaires avec leur ordre chronologique.
    """
    annee_list = []
    
    # Ordre commence √† 0 pour 2021-2022
    for i in range(end_year - start_year + 1):
        annee_start = start_year + i
        annee_end = annee_start + 1
        annee_str = f"{annee_start}-{annee_end}"
        
        annee_list.append({
            'annee': annee_str,
            'ordre_annee': i, # 0, 1, 2, 3, 4, 5...
            'description': f"Ann√©e acad√©mique {annee_str}"
        })
    return annee_list

def import_fixed_references(session: Session):
    """
    Ins√®re les donn√©es de r√©f√©rence fixes (Cycles, Niveaux, Semestres, Types Inscription, Sessions, Types Formation, ANNEES UNIVERSITAIRES).
    """
    print("\n--- 1. Importation des Donn√©es de R√©f√©rence Fixes (LMD, Types & Ann√©es Univ.) ---")
    
    # 1. Cycles
    cycles_data = [{'code': 'L', 'label': 'Licence'}, {'code': 'M', 'label': 'Master'}, {'code': 'D', 'label': 'Doctorat'}]
    for data in cycles_data:
        session.merge(Cycle(**data))
    
    # 2. Niveaux et Semestres 
    niveau_semestre_map = {
        'L1': ('L', ['S01', 'S02']), 'L2': ('L', ['S03', 'S04']), 'L3': ('L', ['S05', 'S06']), 
        'M1': ('M', ['S07', 'S08']), 'M2': ('M', ['S09', 'S10']), 
        'D1': ('D', ['S11', 'S12']), 'D2': ('D', ['S13', 'S14']), 'D3': ('D', ['S15', 'S16']),
    }
    
    for niv_code, (cycle_code, sem_list) in niveau_semestre_map.items():
        session.merge(Niveau(code=niv_code, label=niv_code, cycle_code=cycle_code))
        
        for sem_num in sem_list:
            sem_code_complet = f"{niv_code}_{sem_num}" 
            session.merge(Semestre(
                code_semestre=sem_code_complet,
                numero_semestre=sem_num,
                niveau_code=niv_code 
            ))

    # 3. Modes Inscription
    modes_inscription_data = [{'code': 'CLAS', 'label': 'Classique'}, {'code': 'HYB', 'label': 'Hybride'}]
    for data in modes_inscription_data:
        session.merge(ModeInscription(**data))
        
    # 4. Insertion des Sessions d'Examen
    session_examen_data = [{'code_session': 'N', 'label': 'Normale'}, {'code_session': 'R', 'label': 'Rattrapage'}]
    for sess in session_examen_data:
        session.merge(SessionExamen(**sess))

    # 5. Insertion des Types de Formation
    types_formation_data = [
        {'code': 'FI', 'label': 'Formation Initiale', 'description': 'Formation classique √† temps plein.'},
        {'code': 'FC', 'label': 'Formation Continue', 'description': 'Formation destin√©e aux professionnels en activit√©.'},
        {'code': 'FOAD', 'label': 'Formation √† Distance', 'description': 'Formation ouverte √† distance.'},
    ]
    for data in types_formation_data:
        session.merge(TypeFormation(**data))
        
    # 6. Insertion des Ann√©es Universitaires
    annees_data = _generate_annee_data(start_year=2021, end_year=2026)
    for data in annees_data:
        session.merge(AnneeUniversitaire(**data))
        
    session.commit()
    print("‚úÖ Donn√©es de R√©f√©rence LMD, Types, Sessions et Ann√©es Universitaires ins√©r√©es.")


# ----------------------------------------------------------------------
# FONCTIONS D'IMPORTATION UNITAIRE DE LA STRUCTURE ACAD√âMIQUE
# ----------------------------------------------------------------------

def _import_institutions(session: Session) -> bool:
    """Charge et importe la table Institution."""
    print("\n--- Importation des Institutions ---")
    try:
        df_inst = pd.read_excel(config.INSTITUTION_FILE_PATH)
        df_inst.columns = df_inst.columns.str.lower().str.replace(' ', '_')
        df_inst = df_inst.where(pd.notnull(df_inst), None)
        
        df_inst['institution_id'] = df_inst['institution_id'].astype(str).apply(safe_string)
        # Gestion des doublons et des NaN
        df_inst_clean = df_inst.drop_duplicates(subset=['institution_id']).dropna(subset=['institution_id'])
        
        for _, row in tqdm(df_inst_clean.iterrows(), total=len(df_inst_clean), desc="Institutions"):
            inst_id = row['institution_id']
            logo_path = None
            
            # Logique d'ajout du chemin du logo (inchang√©e)
            logo_base_name = f"{inst_id}"
            for ext in ['.jpg', '.png']:
                potential_path = os.path.join(config.LOGO_FOLDER_PATH, logo_base_name + ext)
                if os.path.exists(potential_path):
                    logo_path = potential_path
                    break 

            session.merge(Institution(
                id_institution=inst_id, 
                nom=safe_string(row['institution_nom']),
                type_institution=safe_string(row['institution_type']),
                logo_path=logo_path 
            ))
        
        # üö® COMMIT CRITIQUE : Permet aux Composantes de voir les Institutions (Cl√© √âtrang√®re)
        session.commit()
        print("‚úÖ Importation des Institutions termin√©e et commit√©e.")
        return True
        
    except Exception as e:
        print(f"‚ùå ERREUR: Impossible de lire ou d'importer le fichier Institutions. {e}", file=sys.stderr)
        session.rollback()
        return False


def _load_and_clean_metadata():
    """Charge et nettoie le fichier de m√©tadonn√©es acad√©miques."""
    try:
        df = pd.read_excel(config.METADATA_FILE_PATH)
        df.columns = df.columns.str.lower().str.replace(' ', '_')
        df = df.where(pd.notnull(df), None)
        print(f"Fichier de m√©tadonn√©es acad√©miques charg√©. {len(df)} lignes trouv√©es.")
        
        # Nettoyage et standardisation des cl√©s
        for col in ['institution_id', 'composante', 'domaine', 'id_mention', 'id_parcours']:
             if col in df.columns:
                 df[col] = df[col].astype(str).apply(safe_string)
        
        return df
        
    except Exception as e:
        print(f"‚ùå ERREUR: Impossible de lire le fichier de m√©tadonn√©es acad√©miques. {e}", file=sys.stderr)
        return None


def _import_composantes(session: Session, df: pd.DataFrame):
    """Importe les Composantes (d√©pend d'Institution)."""
    print("\n--- Importation des Composantes ---")
    if 'composante' not in df.columns:
        print("Colonne 'composante' manquante dans les m√©tadonn√©es.")
        return
        
    # Filtrage des doublons sur la cl√© primaire/unique
    df_composantes = df[['composante', 'label_composante', 'institution_id']].drop_duplicates(
        subset=['composante']).dropna(subset=['composante', 'institution_id'])
    
    for _, row in tqdm(df_composantes.iterrows(), total=len(df_composantes), desc="Composantes"):
        composante_code = row['composante']
        logo_path = None
        
        # Logique d'ajout du chemin du logo
        logo_base_name = f"{composante_code}"
        for ext in ['.jpg', '.png']:
            potential_path = os.path.join(config.LOGO_FOLDER_PATH, logo_base_name + ext)
            if os.path.exists(potential_path):
                logo_path = potential_path
                break 
                
        session.merge(Composante(
            code=composante_code, 
            label=safe_string(row['label_composante']),
            id_institution=row['institution_id'],
            logo_path=logo_path
        ))


def _import_domaines(session: Session, df: pd.DataFrame):
    """Importe les Domaines."""
    print("\n--- Importation des Domaines ---")
    if 'domaine' not in df.columns:
          print("Colonne 'domaine' manquante dans les m√©tadonn√©es.")
          return
          
    df_domaines = df[['domaine', 'label_domaine']].drop_duplicates(subset=['domaine']).dropna(subset=['domaine'])
    
    for _, row in tqdm(df_domaines.iterrows(), total=len(df_domaines), desc="Domaines"):
        session.merge(Domaine(code=row['domaine'], label=safe_string(row['label_domaine'])))


def _import_mentions(session: Session, df: pd.DataFrame):
    """Importe les Mentions (d√©pend de Composante et Domaine)."""
    print("\n--- Importation des Mentions ---")
    if 'id_mention' not in df.columns:
          print("Colonne 'id_mention' manquante dans les m√©tadonn√©es.")
          return
          
    # Correction de la duplication: on utilise id_mention comme cl√© d'unicit√©
    df_mentions_source = df[['mention', 'label_mention', 'id_mention', 'composante', 'domaine']].drop_duplicates(
        subset=['id_mention']).dropna(subset=['id_mention', 'composante', 'domaine', 'mention'])
    
    for _, row in tqdm(df_mentions_source.iterrows(), total=len(df_mentions_source), desc="Mentions"):
        session.merge(Mention(
            id_mention=row['id_mention'], 
            code_mention=safe_string(row['mention']),
            label=safe_string(row['label_mention']),
            composante_code=row['composante'], 
            domaine_code=row['domaine']
        ))
    return df_mentions_source 


def _import_parcours(session: Session, df: pd.DataFrame, df_mentions_source: pd.DataFrame):
    """Importe les Parcours (d√©pend de Mention)."""
    print("\n--- Importation des Parcours ---")
    
    if 'id_parcours' not in df.columns:
          print("Colonne 'id_parcours' manquante dans les m√©tadonn√©es.")
          return
          
    df_parcours = df[['id_parcours', 'parcours', 'label_parcours', 'id_mention', 'date_creation', 'date_fin']].copy()
    
    df_parcours = df_parcours.drop_duplicates(subset=['id_parcours'], keep='first').dropna(subset=['id_parcours', 'id_mention', 'parcours'])

    for _, row in tqdm(df_parcours.iterrows(), total=len(df_parcours), desc="Parcours"):
        
        # Le type Integer pour date_creation/fin est conserv√©
        date_creation_val = int(row['date_creation']) if pd.notna(row['date_creation']) and row['date_creation'] is not None else None
        date_fin_val = int(row['date_fin']) if pd.notna(row['date_fin']) and row['date_fin'] is not None else None
        
        session.merge(Parcours(
            id_parcours=row['id_parcours'], 
            code_parcours=safe_string(row['parcours']), 
            label=safe_string(row['label_parcours']),
            mention_id=row['id_mention'], 
            date_creation=date_creation_val,
            date_fin=date_fin_val
        ))


def import_metadata_to_db(session: Session):
    """
    Orchestre l'importation de la structure acad√©mique.
    """
    print(f"\n--- 2. D√©marrage de l'importation des m√©tadonn√©es ---")

    try:
        # Importe et commit les Institutions (Cl√© parent)
        if not _import_institutions(session):
            return

        df_metadata = _load_and_clean_metadata()
        if df_metadata is None:
            return
        
        # Le reste des tables d√©pend de l'institution d√©j√† commit√©e
        _import_composantes(session, df_metadata) 
        _import_domaines(session, df_metadata)
        
        df_mentions_source = _import_mentions(session, df_metadata) 

        _import_parcours(session, df_metadata, df_mentions_source) 

        session.commit() # Commit final de toutes les tables acad√©miques d√©pendantes
        print("\n‚úÖ Importation des m√©tadonn√©es acad√©miques (Composante, Domaine, Mention, Parcours) termin√©e avec succ√®s.")

    except Exception as e:
        session.rollback()
        print(f"\n‚ùå ERREUR D'IMPORTATION (M√©tadonn√©es): {e}", file=sys.stderr)


# ----------------------------------------------------------------------
# FONCTIONS D'IMPORTATION UNITAIRE DES INSCRIPTIONS
# ----------------------------------------------------------------------

def _load_and_clean_inscriptions():
    """Charge, nettoie et enrichit le fichier d'inscriptions."""
    try:
        df = pd.read_excel(config.INSCRIPTION_FILE_PATH)
        df.columns = df.columns.str.lower().str.replace(' ', '_')
        
        date_cols = ['naissance_date', 'cin_date']
        for col in date_cols:
            df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True).dt.date
            
        df = df.where(pd.notnull(df), None) 
        print(f"Fichier XLSX d'inscriptions charg√©. {len(df)} lignes trouv√©es.")
        
        # Nettoyage et Enrichissement des cl√©s critiques
        if 'id_parcours_caractere' in df.columns:
             df.rename(columns={'id_parcours_caractere': 'id_parcours'}, inplace=True) 
        if 'id_parcours' in df.columns:
             df['id_parcours'] = df['id_parcours'].astype(str).apply(safe_string) 

        if 'semestre_id' in df.columns:
            df.rename(columns={'semestre_id': 'code_semestre'}, inplace=True) 
        elif 'semestre' in df.columns and 'code_semestre' not in df.columns:
            df.rename(columns={'semestre': 'code_semestre'}, inplace=True)

        if 'niveau' in df.columns:
             df.rename(columns={'niveau': 'niveau_code'}, inplace=True)
             
        # 1. Nettoyage/Enrichissement du code_semestre (Doit √™tre L1_S01)
        if 'code_semestre' in df.columns and 'niveau_code' in df.columns:
             df['code_semestre'] = df['code_semestre'].astype(str).apply(safe_string)
             df['niveau_code'] = df['niveau_code'].astype(str).apply(safe_string)
             df['code_semestre'] = df.apply(
                 lambda row: f"{row['niveau_code']}_{row['code_semestre']}" 
                 if pd.notna(row.get('niveau_code')) 
                 and row['code_semestre']
                 and '_' not in str(row['code_semestre'])
                 else row['code_semestre'], 
                 axis=1
             )

        # 2. Standardisation du Mode Inscription
        if 'type_formation' in df.columns and 'code_mode_inscription' not in df.columns: 
             df.rename(columns={'type_formation': 'code_mode_inscription'}, inplace=True)
             
        if 'code_mode_inscription' in df.columns: 
             df.loc[:, 'code_mode_inscription'] = df['code_mode_inscription'].astype(str).str.upper().replace({ 
                 'CLASSIQUE': 'CLAS', 'HYBRIDE': 'HYB'
             })
             df['code_mode_inscription'] = df['code_mode_inscription'].astype(str).apply(safe_string)
        else:
             df['code_mode_inscription'] = 'CLAS' 

        # 3. Standardisation du Type de Formation
        if 'type_formation_code' in df.columns:
             df['code_type_formation'] = df['type_formation_code'].astype(str).apply(safe_string)
        else:
             df['code_type_formation'] = 'FI'
             
        return df
        
    except Exception as e:
        print(f"‚ùå ERREUR: Impossible de lire ou de nettoyer le fichier d'inscriptions. {e}", file=sys.stderr)
        return None

def _import_etudiants(session: Session, df: pd.DataFrame):
    """Importe les √âtudiants (commit par ligne pour la gestion d'erreurs individuelles)."""
    print("\n--- Importation des √âtudiants (Ligne par Ligne) ---")
    
    df_etudiants = df.drop_duplicates(subset=['code_etudiant']).dropna(subset=['code_etudiant', 'nom'])
    etudiant_errors = 0
    
    for index, row in tqdm(df_etudiants.iterrows(), total=len(df_etudiants), desc="Import Etudiants"):
        code_etudiant = row.get('code_etudiant', 'N/A')
        
        try:
            naissance_date_val = row['naissance_date'] if isinstance(row['naissance_date'], date) else None
            cin_date_val = row['cin_date'] if isinstance(row['cin_date'], date) else None
            
            session.merge(Etudiant(
                code_etudiant=safe_string(code_etudiant), 
                numero_inscription=safe_string(row.get('numero_inscription')),
                nom=safe_string(row['nom']), 
                prenoms=safe_string(row['prenoms']),
                sexe=safe_string(row.get('sexe', 'Autre')), 
                naissance_date=naissance_date_val, 
                naissance_lieu=safe_string(row.get('naissance_lieu')),
                nationalite=safe_string(row.get('nationalite')),
                bacc_annee=int(row['bacc_annee']) if pd.notna(row['bacc_annee']) and row['bacc_annee'] is not None else None,
                bacc_serie=safe_string(row.get('bacc_serie')), 
                bacc_centre=safe_string(row.get('bacc_centre')),
                adresse=safe_string(row.get('adresse')), 
                telephone=safe_string(row.get('telephone')), 
                mail=safe_string(row.get('mail')),
                cin=safe_string(row.get('cin')), 
                cin_date=cin_date_val, 
                cin_lieu=safe_string(row.get('cin_lieu'))
            ))
            
            # Commit individuel pour isoler les erreurs d'√©tudiants
            session.commit()
            
        except Exception as e:
            session.rollback()
            etudiant_errors += 1
            e_msg = str(e.orig).lower() if hasattr(e, 'orig') and e.orig else str(e)
            
            print(f"‚ùå [ETUDIANT] Ligne Excel {row.name} ({code_etudiant}) - ERREUR: {e_msg.splitlines()[0]}")
            logging.error(f"ETUDIANT: {code_etudiant} | Erreur: {e_msg} | LIGNE_EXCEL_IDX: {row.name}")
            
    print(f"\n‚úÖ Insertion des √©tudiants termin√©e. {etudiant_errors} erreur(s) individuelle(s) d√©tect√©e(s).")


def _import_inscriptions(session: Session, df: pd.DataFrame):
    """Importe les Inscriptions (commit par lot)."""
    print("\n--- Importation des Inscriptions ---")
    
    cles_requises = ['code_inscription', 'code_etudiant', 'annee_universitaire', 'id_parcours', 'code_semestre', 'code_mode_inscription'] 
    df_inscriptions = df.dropna(subset=cles_requises)
    
    errors_fk, errors_uq, errors_data, errors_other = 0, 0, 0, 0
    
    for index, row in tqdm(df_inscriptions.iterrows(), total=len(df_inscriptions), desc="Import Inscriptions"):
        code_inscription = row.get('code_inscription', 'N/A')
        
        try:
            session.merge(Inscription(
                code_inscription=safe_string(code_inscription), 
                code_etudiant=safe_string(row['code_etudiant']), 
                annee_universitaire=safe_string(row['annee_universitaire']), 
                id_parcours=row['id_parcours'], 
                code_semestre=safe_string(row['code_semestre']), 
                code_mode_inscription=safe_string(row.get('code_mode_inscription', 'CLAS')), 
            ))
            
            if (index + 1) % 500 == 0:
                session.commit()
                
        # Gestion des erreurs (inchang√©e) 
        except IntegrityError as e:
            session.rollback()
            e_msg = str(e.orig).lower()
            if "violates foreign key constraint" in e_msg: errors_fk += 1
            elif "violates unique constraint" in e_msg or "violates not null constraint" in e_msg: errors_uq += 1
            else: errors_other += 1
            logging.error(f"INSCRIPTION (Int√©grit√©): {code_inscription} | D√©tail: {e.orig} | LIGNE_EXCEL_IDX: {row.name}")
        except DataError as e:
            session.rollback()
            errors_data += 1
            logging.error(f"INSCRIPTION (Donn√©es): {code_inscription} | D√©tail: {e.orig} | LIGNE_EXCEL_IDX: {row.name}")
        except Exception as e:
            session.rollback()
            errors_other += 1
            logging.error(f"INSCRIPTION (Autre): {code_inscription} | Erreur: {e} | LIGNE_EXCEL_IDX: {row.name}")
    
    try:
        session.commit()
        print("\n‚úÖ Importation des inscriptions termin√©e.")
        print(f"\n--- R√©capitulatif des erreurs d'insertion ---")
        print(f"Erreurs Cl√© √âtrang√®re/Unique: {errors_fk + errors_uq}")
        print(f"Erreurs Format de Donn√©es: {errors_data}")
        print(f"Autres erreurs: {errors_other}")
        print(f"Voir 'import_errors.log' pour les d√©tails complets.")
    except Exception as e:
        session.rollback()
        print(f"\n‚ùå ERREUR CRITIQUE PENDANT LE COMMIT FINAL: {e}", file=sys.stderr)


def import_inscriptions_to_db(session: Session):
    """
    Orchestre l'importation des donn√©es des √©tudiants et des inscriptions.
    """
    print(f"\n--- 3. D√©marrage de l'importation des inscriptions et √©tudiants ---")
    
    df_inscriptions = _load_and_clean_inscriptions() 
    
    if df_inscriptions is None:
        print("‚ùå Importation des inscriptions annul√©e.")
        return
        
    try:
        _import_etudiants(session, df_inscriptions)
        _import_inscriptions(session, df_inscriptions)

    except Exception as e:
        print(f"\n‚ùå ERREUR dans l'orchestrateur d'inscriptions : {e}", file=sys.stderr)


# ----------------------------------------------------------------------
# FONCTION DE D√âDUCTION DES LIAISONS PARCOURS <-> NIVEAU
# ----------------------------------------------------------------------

def _deduce_parcours_niveaux(session: Session):
    """
    D√©duit et importe les liaisons Parcours <-> Niveau √† partir 
    des inscriptions existantes (Inscription.id_parcours -> Inscription.code_semestre -> Semestre.niveau_code).
    """
    print("\n--- 4. D√©duction et Insertion des Liaisons Parcours <-> Niveau ---")
    
    # 1. Requ√™te SQLAchemy pour trouver toutes les paires uniques (Parcours ID, Niveau Code)
    # L'objectif est de trouver tous les niveaux (L1, M1, etc.) auxquels un parcours a √©t√© associ√© via une inscription.
    try:
        # Jointure: Inscription -> Semestre -> Niveau
        results = (
            session.query(
                Inscription.id_parcours, 
                Niveau.code # Alias√© comme 'niveau_code'
            )
            .join(Semestre, Inscription.code_semestre == Semestre.code_semestre)
            .join(Niveau, Semestre.niveau_code == Niveau.code)
            .distinct()
            .all()
        )
        
        print(f"Trouv√© {len(results)} paires uniques (Parcours, Niveau) √† ins√©rer.")

        if not results:
            print("Aucune paire Parcours/Niveau d√©duite des inscriptions. Importation ignor√©e.")
            return

        # 2. Pr√©paration et Insertion des objets ParcoursNiveau
        records_to_insert = []
        parcours_niveaux_map = {}
        
        # Grouper les niveaux par parcours pour d√©terminer l'ordre
        for parcours_id, niveau_code in results:
            if parcours_id not in parcours_niveaux_map:
                parcours_niveaux_map[parcours_id] = []
            parcours_niveaux_map[parcours_id].append(niveau_code)

        # Les codes de niveau sont ['L1', 'L2', 'L3', 'M1', 'M2', 'D1', 'D2', 'D3']
        # Utiliser un ordre fixe pour les niveaux afin de trier correctement (L1 -> L2 -> L3 -> M1...)
        NIVEAU_ORDRE = {'L1': 1, 'L2': 2, 'L3': 3, 'M1': 4, 'M2': 5, 'D1': 6, 'D2': 7, 'D3': 8}
        
        for parcours_id, niveaux in parcours_niveaux_map.items():
            
            # Trier les niveaux d√©duits selon l'ordre acad√©mique L1, L2, L3...
            niveaux_tries = sorted(niveaux, key=lambda n: NIVEAU_ORDRE.get(n, 99))
            
            for index, niv_code in enumerate(niveaux_tries):
                records_to_insert.append(ParcoursNiveau(
                    id_parcours=parcours_id,
                    code_niveau=niv_code,
                    ordre_niveau_parcours=index + 1 
                ))

        # 3. Insertion en lot
        session.bulk_save_objects(records_to_insert)
        session.commit()
        print(f"‚úÖ Insertion de {len(records_to_insert)} liaisons ParcoursNiveau termin√©e.")

    except Exception as e:
        session.rollback()
        print(f"\n‚ùå ERREUR lors de la d√©duction des ParcoursNiveaux: {e}", file=sys.stderr)
        logging.error(f"DEDUCTION_PN: Erreur fatale lors de la d√©duction. D√©tail: {e}")


# ----------------------------------------------------------------------
# BLOC PRINCIPAL ET ORCHESTRATEUR GLOBAL MIS √Ä JOUR
# ----------------------------------------------------------------------

def import_all_data():
    """
    Orchestre l'ensemble des √©tapes d'importation.
    """
    print("=====================================================")
    print("üöÄ D√âMARRAGE DU PROCESSUS D'IMPORTATION DE DONN√âES üöÄ")
    print("=====================================================")
    
    session = database_setup.get_session() 
    
    try:
        # 1. Importation des r√©f√©rences fixes (Cycles, Niveaux, Semestres, Ann√©es)
        import_fixed_references(session)
        
        # 2. Importation de la structure acad√©mique (Institutions, Composantes, etc.)
        import_metadata_to_db(session) 
        
        # 3. Importation des √©tudiants et des inscriptions
        import_inscriptions_to_db(session)
        
        # 4. üÜï D√âDUCTION ET INSERTION DE LA STRUCTURE TRANSVERSALE
        _deduce_parcours_niveaux(session)
        
        # Un commit final si tout s'est bien pass√© dans les orchestrateurs
        session.commit() # Les sous-fonctions ont g√©r√© les commits n√©cessaires

        print("\n=====================================================")
        print("‚úÖ IMPORTATION GLOBALE TERMIN√âE (V√©rifiez les logs d'erreurs)")
        print("=====================================================")

    except Exception as e:
        session.rollback()
        print(f"\n‚ùå ERREUR FATALE dans l'orchestrateur principal: {e}", file=sys.stderr)
    finally:
        session.close()